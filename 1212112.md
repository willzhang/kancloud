的是非法的本节目录



准备节点

机器名称

服务名称

节点1

Namenode、ResourceManager

节点2

QuorumPeerMain、DataNode、NodeManager、JournalNode

节点3

QuorumPeerMain、DataNode、NodeManager、JournalNode

节点4

QuorumPeerMain、DataNode、NodeManager、JournalNode

配置主机名解析

cat > /etc/hosts <\<EOF

192.168.92.10 server1

192.168.92.11 agent1

192.168.92.12 agent2

192.168.92.13 agent3

EOF

配置节点ssh免密登录

4个节点分别生成公钥和私钥

ssh-keygen -t rsa -P '' -f \~/.ssh/id_rsa

4个节点分别对server1节点免密

ssh-copy-id server1

server1节点分发authorized_keys文件到其他节点

scp /root/.ssh/authorized_keys agent1:/root/.ssh/authorized_keys

scp /root/.ssh/authorized_keys agent2:/root/.ssh/authorized_keys

scp /root/.ssh/authorized_keys agent3:/root/.ssh/authorized_keys

server1节点执行，所有节点公钥写入known_hosts

ssh-keyscan -t ecdsa server1 agent1 agent2 agent3 > /root/.ssh/known_hosts

hadoop01节点执行，分发known_hosts文件到其他节点

scp /root/.ssh/known_hosts agent1:/root/.ssh/

scp /root/.ssh/known_hosts agent2:/root/.ssh/

scp /root/.ssh/known_hosts agent3:/root/.ssh/

安装openjdk

所有节点安装openjdk8

创建安装目录

mkdir /opt/openjdk

下载openjdk

wget https\://github.com/AdoptOpenJDK/openjdk8-binaries/releases/download/jdk8u292-b10/OpenJDK8U-jdk_x64\_linux_hotspot\_8u292b10.tar.gz

解压安装

tar -zxvf OpenJDK8U-jdk_x64\_linux_hotspot\_8u292b10.tar.gz -C /opt/openjdk --strip=1

配置环境变量

cat > /etc/profile.d/openjdk.sh <<'EOF'

export JAVA_HOME=/opt/openjdk

export PATH=$PATH:$JAVA_HOME/bin

EOF



source /etc/profile

确认安装成功

java -version

安装zookeeper集群

所有agent节点安装zookeeper集群

创建zookeeper安装目录

mkdir -p /opt/zookeeper

安装zookeeper

wget https\://mirrors.aliyun.com/apache/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0-bin.tar.gz

tar -zxvf apache-zookeeper-\*-bin.tar.gz -C /opt/zookeeper --strip=1

配置环境变量

cat > /etc/profile.d/zookeeper.sh <<'EOF'

export ZOOKEEPER_HOME=/opt/zookeeper

export PATH=$ZOOKEEPER_HOME/bin:$PATH

EOF



source /etc/profile

创建zookeeper配置文件

cp /opt/zookeeper/conf/{zoo_sample.cfg,zoo.cfg}

查看zookeeper版本

zkServer.sh version

创建zookeeper用户

useradd -r -s /bin/false zookeeper

chown -R zookeeper: /opt/zookeeper

使用systemd管理zookeeper服务

cat > /usr/lib/systemd/system/zookeeper.service <\<EOF

\[Unit]

Description=Zookeeper Distributed Coordination Server

Documentation=http\://zookeeper.apache.org

After=network.target

Wants=network.target



\[Service]

Type=forking

User=zookeeper

Group=zookeeper

WorkingDirectory=/opt/zookeeper

PIDFile=/opt/zookeeper/data/zookeeper_server.pid

Environment=JAVA_HOME=/opt/openjdk

ExecStart=/opt/zookeeper/bin/zkServer.sh start

ExecStop=/opt/zookeeper/bin/zkServer.sh stop

Restart=on-failure

TimeoutSec=20

SuccessExitStatus=SIGKILL



\[Install]

WantedBy=multi-user.target

EOF

启动zookeeper服务

systemctl enable --now zookeeper

安装hadoop

所有节点安装hadoop

创建安装目录

mkdir -p /opt/hadoop

阿里云下载hadoop：

wget https\://mirrors.aliyun.com/apache/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz -P /tmp

解压hadoop

tar -zxvf /tmp/hadoop-3.3.0.tar.gz -C /opt/hadoop --strip=1

配置环境变量

cat > /etc/profile.d/hadoop.sh <<'EOF'

export HADOOP_HOME=/opt/hadoop

export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

EOF



source /etc/profile

查看hadoop版本

hadoop version

修改hadoop配置文件

修改hadoop-env.sh

cd $HADOOP_HOME/etc/hadoop

修改环境变量JAVA_HOME为绝对路径，并将用户指定为root。

echo "export JAVA_HOME=/opt/openjdk" >> hadoop-env.sh

echo "export HDFS_NAMENODE_USER=root" >> hadoop-env.sh

echo "export HDFS_SECONDARYNAMENODE_USER=root" >> hadoop-env.sh

echo "export HDFS_DATANODE_USER=root" >> hadoop-env.sh

修改yarn-env.sh

修改用户为root。

echo "export YARN_REGISTRYDNS_SECURE_USER=root" >> yarn-env.sh

echo "export YARN_RESOURCEMANAGER_USER=root" >> yarn-env.sh

echo "export YARN_NODEMANAGER_USER=root" >> yarn-env.sh

修改core-site.xml

cat > $HADOOP_HOME/etc/hadoop/core-site.xml <\<EOF

\<?xml version="1.0" encoding="UTF-8"?>

\<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>



\<configuration>

\<property>

    \<name>fs.defaultFS\</name>

    \<value>hdfs\://server1:9000\</value>

\</property>

\<property>

  \<name>hadoop.tmp.dir\</name>

  \<value>/home/hadoop_tmp_dir\</value>

\</property>

\<property>

   \<name>ipc.client.connect.max.retries\</name>

   \<value>100\</value>

\</property>

\<property>

   \<name>ipc.client.connect.retry.interval\</name>

   \<value>10000\</value>

\</property>

\<property>

   \<name>hadoop.proxyuser.root.hosts\</name>

   \<value>\*\</value>

\</property>

\<property>

   \<name>hadoop.proxyuser.root.groups\</name>

   \<value>\*\</value>

\</property>

\</configuration>

EOF

在节点server1上创建目录。

mkdir /home/hadoop_tmp_dir

修改hdfs-site.xml

cat > $HADOOP_HOME/etc/hadoop/hdfs-site.xml <\<EOF

\<?xml version="1.0" encoding="UTF-8"?>

\<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>



\<configuration>

\<property>

    \<name>dfs.replication\</name>

    \<value>1\</value>

\</property>

\<property>

    \<name>dfs.namenode.name.dir\</name>

    \<value>/data/data1/hadoop/nn\</value>

\</property>

\<property>

    \<name>dfs.datanode.data.dir\</name>

	\<value>/data/data1/hadoop/dn,/data/data2/hadoop/dn,/data/data3/hadoop/dn,/data/data4/hadoop/dn,/data/data5/hadoop/dn,/data/data6/hadoop/dn,/data/data7/hadoop/dn,/data/data8/hadoop/dn,/data/data9/hadoop/dn,/data/data10/hadoop/dn,/data/data11/hadoop/dn,/data/data12/hadoop/dn\</value>

\</property>

\<property>

    \<name>dfs.http.address\</name>

    \<value>server1:50070\</value>

\</property>

\<property>

    \<name>dfs.namenode.http-bind-host\</name>

    \<value>0.0.0.0\</value>

\</property>

\<property>

    \<name>dfs.datanode.handler.count\</name>

    \<value>600\</value>

\</property>

\<property>

    \<name>dfs.namenode.handler.count\</name>

    \<value>600\</value>

\</property>

\<property>

    \<name>dfs.namenode.service.handler.count\</name>

    \<value>600\</value>

\</property>

\<property>

    \<name>ipc.server.handler.queue.size\</name>

    \<value>300\</value>

\</property>

\<property>

    \<name>dfs.webhdfs.enabled\</name>

    \<value>true\</value>

\</property>

\</configuration>

EOF

节点agent1、agent2、agent3分别创建dfs.datanode.data.dir对应目录。

mkdir -p /data/data{1,2,3,4,5,6,7,8,9,10,11,12}/hadoop

修改mapred-site.xml

cat > $HADOOP_HOME/etc/hadoop/mapred-site.xml <\<EOF

\<?xml version="1.0"?>

\<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>



\<configuration>

\<property>

    \<name>mapreduce.framework.name\</name>

    \<value>yarn\</value>

    \<final>true\</final>

    \<description>The runtime framework for executing MapReduce jobs\</description>

\</property>

\<property>

    \<name>mapreduce.job.reduce.slowstart.completedmaps\</name>

    \<value>0.88\</value>

\</property>

\<property>

    \<name>mapreduce.application.classpath\</name>

    \<value>

        /opt/hadoop/etc/hadoop,

        /opt/hadoop/share/hadoop/common/\*,

        /opt/hadoop/share/hadoop/common/lib/\*,

        /opt/hadoop/share/hadoop/hdfs/\*,

        /opt/hadoop/share/hadoop/hdfs/lib/\*,

        /opt/hadoop/share/hadoop/mapreduce/\*,

        /opt/hadoop/share/hadoop/mapreduce/lib/\*,

        /opt/hadoop/share/hadoop/yarn/\*,

        /opt/hadoop/share/hadoop/yarn/lib/\*

    \</value>

\</property>

\<property>

    \<name>mapreduce.map.memory.mb\</name>

    \<value>6144\</value>

\</property>

\<property>

    \<name>mapreduce.reduce.memory.mb\</name>

    \<value>6144\</value>

 \</property>

 \<property>

    \<name>mapreduce.map.java.opts\</name>

    \<value>-Xmx5530m\</value>

\</property>

\<property>

    \<name>mapreduce.reduce.java.opts\</name>

    \<value>-Xmx2765m\</value>

\</property>

\<property>

    \<name>mapred.child.java.opts\</name>

    \<value>-Xmx2048m -Xms2048m\</value>

\</property>

\<property>

    \<name>mapred.reduce.parallel.copies\</name>

    \<value>20\</value>

\</property>

\<property>

    \<name>yarn.app.mapreduce.am.env\</name>

    \<value>HADOOP_MAPRED_HOME=/opt/hadoop\</value>

\</property>

\<property>

    \<name>mapreduce.map.env\</name>

    \<value>HADOOP_MAPRED_HOME=/opt/hadoop\</value>

\</property>

\<property>

    \<name>mapreduce.reduce.env\</name>

    \<value>HADOOP_MAPRED_HOME=/opt/hadoop\</value>

\</property>

\</configuration>

EOF

修改yarn-site.xml

cat > $HADOOP_HOME/etc/hadoop/yarn-site.xml <\<EOF

\<?xml version="1.0"?>



\<configuration>

\<property>

    \<name>yarn.nodemanager.aux-services\</name>

    \<value>mapreduce_shuffle\</value>

    \<final>true\</final>

\</property>

\<property>

    \<name>yarn.nodemanager.aux-services\</name>

    \<value>mapreduce_shuffle\</value>

\</property>

\<property>

    \<name>yarn.resourcemanager.hostname\</name>

    \<value>server1\</value>

\</property>

\<property>

    \<name>yarn.resourcemanager.bind-host\</name>

    \<value>0.0.0.0\</value>

\</property>

\<property>

    \<name>yarn.scheduler.maximum-allocation-mb\</name>

    \<value>65536\</value>

\</property>

\<property>

    \<name>yarn.nodemanager.resource.memory-mb\</name>

    \<value>102400\</value>

\</property>

\<property>

    \<name>yarn.nodemanager.resource.cpu-vcores\</name>

    \<value>48\</value>

\</property>

\<property>

    \<name>yarn.log-aggregation-enable\</name>

    \<value>true\</value>

\</property>

\<property>

    \<name>yarn.client.nodemanager-connect.max-wait-ms\</name>

    \<value>300000\</value>

\</property>

\<property>

    \<name>yarn.nodemanager.vmem-pmem-ratio\</name>

    \<value>7.1\</value>

\</property>

\<property>

    \<name>yarn.nodemanager.vmem-check-enabled\</name>

    \<value>false\</value>

\</property>

\<property>

    \<name>yarn.nodemanager.pmem-check-enabled\</name>

    \<value>false\</value>

\</property>

\<property>

    \<name>yarn.scheduler.minimum-allocation-mb\</name>

    \<value>3072\</value>

\</property>

\<property>

    \<name>yarn.app.mapreduce.am.resource.mb\</name>

    \<value>3072\</value>

\</property>

\<property>

    \<name>yarn.scheduler.maximum-allocation-vcores\</name>

    \<value>48\</value>

\</property>

\<property>

    \<name>yarn.application.classpath\</name>

    \<value>

        /opt/hadoop/etc/hadoop,

        /opt/hadoop/share/hadoop/common/\*,

        /opt/hadoop/share/hadoop/common/lib/\*,

        /opt/hadoop/share/hadoop/hdfs/\*,

        /opt/hadoop/share/hadoop/hdfs/lib/\*,

        /opt/hadoop/share/hadoop/mapreduce/\*,

        /opt/hadoop/share/hadoop/mapreduce/lib/\*,

        /opt/hadoop/share/hadoop/yarn/\*,

        /opt/hadoop/share/hadoop/yarn/lib/\*

    \</value>

\</property>

\</configuration>

EOF

节点agent1、agent2、agent3分别创建yarn.nodemanager.local-dirs对应目录。

mkdir -p /data/data{1,2,3,4,5,6,7,8,9,10,11,12}/yarn

修改slaves 或 workers

确认Hadoop版本，3.x以下的版本编辑slaves文件，3.x及以上的编辑workers文件。

修改workers文件，只保存所有agent节点的IP地址（可用主机名代替），其余内容均删除。

cat > $HADOOP_HOME/etc/hadoop/workers.xml <\<EOF

agent1

agent2

agent3

EOF

同步配置到其它节点

所有节点依次创建journaldata目录。

mkdir -p $HADOOP_HOME/journaldata

拷贝配置文件到agent1、agent2、agent3节点对应目录。

scp -r /opt/hadoop/etc/hadoop/\* root\@agent1:/opt/hadoop/etc/hadoop/

scp -r /opt/hadoop/etc/hadoop/\* root\@agent2:/opt/hadoop/etc/hadoop/

scp -r /opt/hadoop/etc/hadoop/\* root\@agent3:/opt/hadoop/etc/hadoop/

启动Hadoop集群

1、启动ZooKeeper集群。

分别在agent1，agent2，agent3节点上启动ZooKeeper。

cd /usr/local/zookeeper/bin

./zkServer.sh start



jps

2、启动JournalNode。

分别在agent1，agent2，agent3节点上启动JournalNode。

只在第一次进行格式化操作时，需要执行2-4，完成格式化后，下次启动集群，只需要执行1、5、6。

hdfs --daemon start journalnode

确认启动成功

\[root\@agent1 \~]# jps

7392 JournalNode

7520 Jps

6508 QuorumPeerMain

3、格式化HDFS，在server1节点上格式化HDFS。

hdfs namenode -format

格式化后集群会根据core-site.xml配置的hadoop.tmp.dir参数生成目录，本文档配置目录为“/home/hadoop_tmp”。

4、格式化ZKFC，在server1节点上格式化ZKFC。（省略）

hdfs zkfc -formatZK

5、启动HDFS，在server1节点上启动HDFS。

start-dfs.sh

6、启动Yarn，在server1节点上启动Yarn。

start-yarn.sh

7、观察进程是否都正常启动。

此操作在所有节点上执行，其中server节点应启动的进程以server1的截图为准，agent节点应启动的进程以agent1的截图为准。

server1节点

\[root\@server1 \~]# jps

7703 NameNode

7944 SecondaryNameNode

8825 ResourceManager

9150 Jps

agent节点

\[root\@agent1 \~]# jps

7392 JournalNode

8536 Jps

6508 QuorumPeerMain

8446 NodeManager

7631 DataNode

验证Hadoop

在浏览器中输入URL地址，访问Hadoop Web页面，URL格式为“http\://server1:50070”。

其中，“server1”填写server进程所在节点的IP地址。通过观察Live Nodes是否与agent数目相等（本文是3）、Dead Nodes是否为0，判断集群是否正常启动。

